{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "\n",
    "boto_session = boto3.Session(profile_name='dev-profile')\n",
    "client = boto_session.client(\"sagemaker-runtime\")\n",
    "\n",
    "# This has to be the same name as used in terraform to name the endpoint\n",
    "ENDPOINT_NAME = \"mistral-model-endpoint\"\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "#os.environ[\"HF_API_TOKEN\"] = 'token'\n",
    "# HF_API_TOKEN= os.environ[\"HF_API_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"<s>[INST] Write a poem about a cat named Homer. [/INST] In a quiet little nook, where the sun gently shed,\\nLive stories, tales and lore, of a feline named Homer, a thread.\\nA coat of midnight hues, beneath the crescent moon's pull,\\nHis eyes, emerald orbs, gleaming with ageless allure, and a brood so full.\\n\\nWith whiskers twitching, and ears that perked so fine,\\nHis graceful fingers swiped\"}]\n"
     ]
    }
   ],
   "source": [
    "# first test to verify endpoint is functioning correctly\n",
    "user_message = \"Write a poem about a cat named Homer.\"\n",
    "prompt = f\"<s>[INST] {user_message} [/INST]\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "}\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=ENDPOINT_NAME,\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/json\",\n",
    "    Body=json.dumps(payload)\n",
    ")\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/katie/Coding/NASA_RAG/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Finally, we need to combine the llm_chain with the retriever to create a RAG chain.\n",
    "# We pass the original question through to the final generation step, as well as the retrieved context docs\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "db = FAISS.load_local(folder_path=\"faiss_db/\", embeddings=embeddings, index_name=\"nasa_index\", allow_dangerous_deserialization=True)\n",
    "\n",
    "# This retriever returns the top 5 similar chunks\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "\n",
    "# TODO figure out how to adapt it\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "    \n",
    "    # this should be adapted to include context\n",
    "    def transform_input(self, prompt, model_kwargs):\n",
    "        input_str = json.dumps({\n",
    "                \"inputs\" : prompt,\n",
    "                \"parameters\" : {**model_kwargs}\n",
    "            })\n",
    "        return input_str.encode('utf-8')\n",
    "    \n",
    "    def transform_output(self, output):\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[0][\"generation\"][\"content\"]\n",
    "    \n",
    "\n",
    "content_handler = ContentHandler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import SagemakerEndpoint\n",
    "\n",
    "llm=SagemakerEndpoint(\n",
    "    client = client,\n",
    "    endpoint_name=ENDPOINT_NAME, \n",
    "    region_name='eu-west-1', \n",
    "    model_kwargs={\"max_new_tokens\": 100, \"top_p\": 0.9, \"temperature\": 0.2},\n",
    "    content_handler=content_handler\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# Create a prompt template\n",
    "# this should follow the format of the model, so make sure to use the appropriate formatting.\n",
    "prompt_template = \"\"\"\n",
    "<|system|>\n",
    "The following is a friendly conversation between a human and an AI. \n",
    "The AI is talkative and provides lots of specific details from its context.\n",
    "If the AI does not know the answer to a question, it truthfully says it does not know. \n",
    "Use the following context to help:\n",
    "\n",
    "{context}\n",
    "\n",
    "</s>\n",
    "{question}\n",
    "</s>\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "llm_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# You can also use tokenizer.apply_chat_template to convert a list of messages (as dicts: {'role': 'user', 'content': '(...)'})\n",
    "# into a string with the appropriate chat format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunnablePassthrough Usage: The query is then passed along using RunnablePassthrough(). \n",
    "# This function is a part of LangChainâ€™s API and is used to pass the query to the next step in the chain.\n",
    "rag_chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | llm_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:An error occurred: Error raised by inference endpoint: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (422) from primary with message \"{\"error\":\"Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4096. Given: 3651 `inputs` tokens and 4000 `max_new_tokens`\",\"error_type\":\"validation\"}\". See https://eu-west-1.console.aws.amazon.com/cloudwatch/home?region=eu-west-1#logEventViewer:group=/aws/sagemaker/Endpoints/mistral-model-endpoint in account 051285996764 for more information.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "# A RAG response considering our NASA context\n",
    "try:\n",
    "    # Need to reduce size of chunks\n",
    "    response = rag_chain.invoke(\"What can you tell me about the latest space discoveries?\")\n",
    "    logging.info(response)\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
