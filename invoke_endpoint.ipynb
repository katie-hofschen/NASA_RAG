{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/katie/Library/Application Support/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "boto_session = boto3.Session(profile_name='dev-profile')\n",
    "client = boto_session.client(\"sagemaker-runtime\")\n",
    "# This has to be the same name as used in terraform to name the endpoint\n",
    "# TODO set it as env var endpoint_name = os.environ[\"LLAMA_2_70B_ENDPOINT\"]\n",
    "ENDPOINT_NAME = \"mistral-model-endpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"score\":0.506294310092926,\"start\":0,\"end\":26,\"answer\":\"you are a smart dictionary\"}\n"
     ]
    }
   ],
   "source": [
    "# first test to verify endpoint is functioning correctly\n",
    "body={\"question\":\"List 3 synonyms for the word tiny.\", \"context\":\"Synonyms for tiny are small, minute, small-scale, mini, baby, fun-size, petite.\"}\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "            EndpointName=ENDPOINT_NAME,\n",
    "            ContentType=\"application/json\",\n",
    "            Accept=\"application/json\",\n",
    "            Body=json.dumps(body),\n",
    "        )\n",
    "print(response['Body'].read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try: aws sagemaker-runtime invoke-endpoint --endpoint-name stablelm-zephyr-3b-endpoint --body '{\"inputs\": \"What is the capital of France?\"}' --content-type application/json output.json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from transformers import DistilBertTokenizer, TFDistilBertForQuestionAnswering     \n",
    "import tensorflow as tf     \n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-distilled-squad\")     \n",
    "model = TFDistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased-distilled-squad\")     \n",
    "\n",
    "question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"     \n",
    "\n",
    "inputs = tokenizer(question, text, return_tensors=\"tf\")      \n",
    "outputs = model(**inputs)     \n",
    "\n",
    "answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])     \n",
    "answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])     \n",
    "\n",
    "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]     \n",
    "tokenizer.decode(predict_answer_tokens)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt, model_kwargs):\n",
    "        input_str = json.dumps(\n",
    "            {\n",
    "                \"inputs\" : [[\n",
    "                    {\"role\" : \"system\",\n",
    "                    \"content\" : \"You are a kind robot.\"},\n",
    "                    {\"role\" : \"user\", \n",
    "                    \"content\" : prompt}]],\n",
    "                \"parameters\" : {**model_kwargs}\n",
    "            })\n",
    "        return input_str.encode('utf-8')\n",
    "    \n",
    "    def transform_output(self, output):\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[0][\"generation\"][\"content\"]\n",
    "    \n",
    "\n",
    "content_handler = ContentHandler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import SagemakerEndpoint\n",
    "\n",
    "model=SagemakerEndpoint(\n",
    "     endpoint_name=ENDPOINT_NAME, \n",
    "     region_name='eu-west-1', \n",
    "     model_kwargs={\"max_new_tokens\": 700, \"top_p\": 0.9, \"temperature\": 0.2},\n",
    "     content_handler=content_handler\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.2,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=400,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# Create a prompt template\n",
    "# this should follow the format of the model, so make sure to use the appropriate formatting.\n",
    "prompt_template = \"\"\"\n",
    "<|system|>\n",
    "The following is a friendly conversation between a human and an AI. \n",
    "The AI is talkative and provides lots of specific details from its context.\n",
    "If the AI does not know the answer to a question, it truthfully says it does not know. \n",
    "Use the following context to help:\n",
    "\n",
    "{context}\n",
    "\n",
    "</s>\n",
    "<|user|>\n",
    "{question}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\n",
    " \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "llm_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# You can also use tokenizer.apply_chat_template to convert a list of messages (as dicts: {'role': 'user', 'content': '(...)'})\n",
    "# into a string with the appropriate chat format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Finally, we need to combine the llm_chain with the retriever to create a RAG chain.\n",
    "# We pass the original question through to the final generation step, as well as the retrieved context docs\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "db = FAISS.load_local(folder_path=\"faiss_db/\", embeddings=embeddings, index_name=\"nasa_index\", allow_dangerous_deserialization=True)\n",
    "\n",
    "# This retriever returns the top 5 similar chunks\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "\n",
    "rag_chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | llm_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A RAG response considering our NASA context\n",
    "rag_chain.invoke(\"What can you tell me about the latest space discoveries?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
